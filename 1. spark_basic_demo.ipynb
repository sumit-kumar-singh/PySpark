{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.11.29.55:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking spark using for SparkContext\n",
    "# SparkContext, SparkUI, Spark Version and AppName gets ouputted.\n",
    "# SparkContext is entry point of any Spark application and acts as Master Node\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing row library \n",
    "# helps in fetching rows from the datasets\n",
    "from pyspark.sql.types import Row\n",
    "# importing datatime library\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext has method that covert any list into RDD using sc.paralleize()\n",
    "list_data = sc.parallelize([1, 'Alice', 50])\n",
    "list_data\n",
    "# outputting info as RDD created successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting counts of elements in the list\n",
    "list_data.count()\n",
    "# we have three items in the list list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting first item\n",
    "list_data.first()\n",
    "# displaying 0th index data from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'Alice']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using take to get set of rows or values from starting postion/index\n",
    "list_data.take(2)\n",
    "# returns two items from start position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'Alice', 50]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting all data from list using collect\n",
    "list_data.collect()\n",
    "# returns all the data items in the list\n",
    "\n",
    "# All these operations done like count, collect, take these are actions and quite expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-be4e95b367d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# in actual Saprk2 has dataframes so we can covert RDDs into dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# only those RDD having proper schema (tabular form) can be converted into dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# getting error as [Can not infer schema for type: <class 'int'>] as list_data list has no schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installation/spark-install/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installation/spark-install/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installation/spark-install/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installation/spark-install/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installation/spark-install/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'int'>"
     ]
    }
   ],
   "source": [
    "# in actual Saprk2 has dataframes so we can covert RDDs into dataframes\n",
    "# only those RDD having proper schema (tabular form) can be converted into dataframe\n",
    "df = list_data.toDF()\n",
    "# getting error as [Can not infer schema for type: <class 'int'>] as list_data list has no schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[8] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets work with structured/schema data\n",
    "records = sc.parallelize([[1, 'Alice', 50], [2, 'Bob', 75], [3, 'Tuna', 100]])\n",
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'Alice', 50], [2, 'Bob', 75], [3, 'Tuna', 100]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying RDD records\n",
    "records.collect()\n",
    "# this is structured data as each records have same entry as id, name and pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint, _2: string, _3: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting structured data RDD to dataframe\n",
    "dataframe = records.toDF()\n",
    "dataframe\n",
    "# spark first infer the schema for the resultant dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| _1|   _2| _3|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 50|\n",
      "|  2|  Bob| 75|\n",
      "|  3| Tuna|100|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show methods to view the data\n",
    "dataframe.show()\n",
    "# showing data in tabular form as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[19] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here label of data is not readable so use row object to create labeled dataframe\n",
    "row = sc.parallelize([Row(id=1,\n",
    "                           name='Alice',\n",
    "                           score=50\n",
    "                          ),\n",
    "                       Row(id=1,\n",
    "                           name='Bob',\n",
    "                           score=100\n",
    "                          ),\n",
    "                       Row(id=1,\n",
    "                           name='Charlie',\n",
    "                           score=80\n",
    "                          )\n",
    "                      ])\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='Alice', score=50),\n",
       " Row(id=1, name='Bob', score=100),\n",
       " Row(id=1, name='Charlie', score=80)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collecting data\n",
    "row.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, score: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting it into dataframe\n",
    "data = row.toDF()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|   name|score|\n",
      "+---+-------+-----+\n",
      "|  1|  Alice|   50|\n",
      "|  1|    Bob|  100|\n",
      "|  1|Charlie|   80|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# showing dataframe\n",
    "data.show()\n",
    "# now the column name of the table has changed to more readable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[30] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working with complex records\n",
    "complex_record = sc.parallelize([Row(\n",
    "                                    col_list = [1, 50],\n",
    "                                    col_dict = {'name':'Bob', 'score':100},\n",
    "                                    col_row = Row(a=10, b=20, c=30),\n",
    "                                    col_time = datetime(2019, 7, 19, 1, 7)\n",
    "                                 ),\n",
    "                                 Row(\n",
    "                                    col_list = [1, 30],\n",
    "                                    col_dict = {'name':'Marry', 'score':100, 'age':30},\n",
    "                                    col_row = Row(a=50, b=20, c=30),\n",
    "                                    col_time = datetime(2019, 9, 10, 1, 3)\n",
    "                                 ),\n",
    "                                 Row(\n",
    "                                    col_list = [1, 40],\n",
    "                                    col_dict = {'name':'Bob', 'score':100},\n",
    "                                    col_row = Row(a=90, b=20, c=30),\n",
    "                                    col_time = datetime(2019, 7, 19, 1, 7)\n",
    "                                 ),\n",
    "                                 Row(\n",
    "                                    col_list = [1, 50, 100],\n",
    "                                    col_dict = {'name':'July', 'score':100},\n",
    "                                    col_row = Row(a=600, b=20, c=30),\n",
    "                                    col_time = datetime(2019, 5, 22, 6, 2)\n",
    "                                 )\n",
    "                                ])\n",
    "complex_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col_dict: map<string,string>, col_list: array<bigint>, col_row: struct<a:bigint,b:bigint,c:bigint>, col_time: timestamp]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting RDD into dataframe\n",
    "complex_df = complex_record.toDF()\n",
    "complex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------+-------------------+\n",
      "|            col_dict|    col_list|      col_row|           col_time|\n",
      "+--------------------+------------+-------------+-------------------+\n",
      "|[name -> Bob, sco...|     [1, 50]| [10, 20, 30]|2019-07-19 01:07:00|\n",
      "|[name -> Marry, s...|     [1, 30]| [50, 20, 30]|2019-09-10 01:03:00|\n",
      "|[name -> Bob, sco...|     [1, 40]| [90, 20, 30]|2019-07-19 01:07:00|\n",
      "|[name -> July, sc...|[1, 50, 100]|[600, 20, 30]|2019-05-22 06:02:00|\n",
      "+--------------------+------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the entire data table\n",
    "complex_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(a=10, b=20, c=30)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working on particular cell of dataframe\n",
    "cell_data = complex_df.collect()[0][2]\n",
    "cell_data\n",
    "# getting data from 1st row and 2nd column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 50, 100]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3rd row and 1st column data\n",
    "cell_list = complex_df.collect()[3][1]\n",
    "cell_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 50, 100, 200]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding new item to list\n",
    "cell_list.append(200)\n",
    "cell_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------+-------------------+\n",
      "|            col_dict|    col_list|      col_row|           col_time|\n",
      "+--------------------+------------+-------------+-------------------+\n",
      "|[name -> Bob, sco...|     [1, 50]| [10, 20, 30]|2019-07-19 01:07:00|\n",
      "|[name -> Marry, s...|     [1, 30]| [50, 20, 30]|2019-09-10 01:03:00|\n",
      "|[name -> Bob, sco...|     [1, 40]| [90, 20, 30]|2019-07-19 01:07:00|\n",
      "|[name -> July, sc...|[1, 50, 100]|[600, 20, 30]|2019-05-22 06:02:00|\n",
      "+--------------------+------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_df.show()\n",
    "# here we can see the original df is not changing by append operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1, 50], Row(a=10, b=20, c=30)),\n",
       " ([1, 30], Row(a=50, b=20, c=30)),\n",
       " ([1, 40], Row(a=90, b=20, c=30)),\n",
       " ([1, 50, 100], Row(a=600, b=20, c=30))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting specific column using map()\n",
    "complex_df.rdd\\\n",
    "            .map(lambda x: (x.col_list, x.col_row))\\\n",
    "            .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|    col_list|      col_row|\n",
      "+------------+-------------+\n",
      "|     [1, 50]| [10, 20, 30]|\n",
      "|     [1, 30]| [50, 20, 30]|\n",
      "|     [1, 40]| [90, 20, 30]|\n",
      "|[1, 50, 100]|[600, 20, 30]|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# showing specific column using select\n",
    "complex_df.select(\n",
    "            'col_list',\n",
    "            'col_row'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_dict</th>\n",
       "      <th>col_list</th>\n",
       "      <th>col_row</th>\n",
       "      <th>col_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'name': 'Bob', 'score': '100'}</td>\n",
       "      <td>[1, 50]</td>\n",
       "      <td>(10, 20, 30)</td>\n",
       "      <td>2019-07-19 01:07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'name': 'Marry', 'score': '100', 'age': '30'}</td>\n",
       "      <td>[1, 30]</td>\n",
       "      <td>(50, 20, 30)</td>\n",
       "      <td>2019-09-10 01:03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'name': 'Bob', 'score': '100'}</td>\n",
       "      <td>[1, 40]</td>\n",
       "      <td>(90, 20, 30)</td>\n",
       "      <td>2019-07-19 01:07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'name': 'July', 'score': '100'}</td>\n",
       "      <td>[1, 50, 100]</td>\n",
       "      <td>(600, 20, 30)</td>\n",
       "      <td>2019-05-22 06:02:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         col_dict      col_list  \\\n",
       "0                 {'name': 'Bob', 'score': '100'}       [1, 50]   \n",
       "1  {'name': 'Marry', 'score': '100', 'age': '30'}       [1, 30]   \n",
       "2                 {'name': 'Bob', 'score': '100'}       [1, 40]   \n",
       "3                {'name': 'July', 'score': '100'}  [1, 50, 100]   \n",
       "\n",
       "         col_row            col_time  \n",
       "0   (10, 20, 30) 2019-07-19 01:07:00  \n",
       "1   (50, 20, 30) 2019-09-10 01:03:00  \n",
       "2   (90, 20, 30) 2019-07-19 01:07:00  \n",
       "3  (600, 20, 30) 2019-05-22 06:02:00  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting dataframe to pandas dataframe\n",
    "panda_data = complex_df.toPandas()\n",
    "panda_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f67aa3d5f98>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # SQLcontext is on the top of SparkContext\n",
    "sqlContext =  SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# range function for creating dataframe\n",
    "sql_df = sqlContext.range(5)\n",
    "sql_df\n",
    "# creating a dataframe inference with datatype/schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying dataframe\n",
    "sql_df.show()\n",
    "# single column dataframe is created using range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all methods are same for sqlcontext also\n",
    "sql_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='Alice', score=50),\n",
       " Row(id=1, name='Bob', score=100),\n",
       " Row(id=1, name='Charlie', score=80)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating list for given data\n",
    "sql_data = [Row(id=1, name='Alice', score=50\n",
    "            ),\n",
    "            Row(id=1, name='Bob', score=100\n",
    "            ),\n",
    "            Row(id=1, name='Charlie', score=80\n",
    "            )\n",
    "           ]\n",
    "sql_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|   name|score|\n",
      "+---+-------+-----+\n",
      "|  1|  Alice|   50|\n",
      "|  1|    Bob|  100|\n",
      "|  1|Charlie|   80|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# converting it into sqlcontest\n",
    "sqlContext.createDataFrame(sql_data).show()\n",
    "# directly setting up dataframe from given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------+-------------------+\n",
      "|            col_dict|    col_list|      col_row|           col_time|\n",
      "+--------------------+------------+-------------+-------------------+\n",
      "|[name -> Bob, sco...|     [1, 50]| [10, 20, 30]|2019-07-19 01:07:00|\n",
      "|[name -> Marry, s...|     [1, 30]| [50, 20, 30]|2019-09-10 01:03:00|\n",
      "|[name -> Bob, sco...|     [1, 40]| [90, 20, 30]|2019-07-19 01:07:00|\n",
      "|[name -> July, sc...|[1, 50, 100]|[600, 20, 30]|2019-05-22 06:02:00|\n",
      "+--------------------+------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating RDD in sparkcontest\n",
    "pandas_df = sqlContext.createDataFrame(panda_data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
